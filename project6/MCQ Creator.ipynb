{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3716f98",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">MCQ Creator App</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af634de",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "      ##### Install & Import Dependencies\n",
    "      ##### Load Documents\n",
    "      ##### Transformer Documents\n",
    "      ##### Generate Text Embeddings\n",
    "      ##### Vector store - PINECONE\n",
    "      ##### Retrieve Answers\n",
    "      ##### Structure the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3323b",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e48a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install unstructured\n",
    "# ! pip install tiktoken\n",
    "# ! pip install pinecone-client\n",
    "# ! pip install pypdf\n",
    "# ! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0348a82f",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b382dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone \n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb6019",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "The code sets environment variables for accessing Hugging Face Hub API using respective API keys<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e40e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1fc352",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e5fd9",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Loads PDF files available in a directory with pypdf<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffc8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read documents\n",
    "def load_docs(directory):\n",
    "  loader = PyPDFDirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d28269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing the directory to the 'load_docs' function\n",
    "directory = 'Docs/'\n",
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4770d",
   "metadata": {},
   "source": [
    "## Transform Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79688cbe",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Split document Into Smaller Chunks<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63503325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will split the documents into chunks\n",
    "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a00b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = split_docs(documents)\n",
    "# print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25244e9c",
   "metadata": {},
   "source": [
    "## Generate Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe08aa",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Hugging Face LLM for creating Embeddings for documents/Text<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60744d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce78a6",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Let's test our Embeddings model for a sample text<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38387cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(\"Hello Buddy\")\n",
    "# len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d422eaa",
   "metadata": {},
   "source": [
    "## Vector store - PINECONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a7621",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Pinecone allows for data to be uploaded into a vector database and true semantic search can be performed.<br><br> Not only is conversational data highly unstructured, but it can also be complex. Vector search and vector databases allows for similarity searches.<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84fd4c",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "We will initialize Pinecone and create a Pinecone index by passing our documents, embeddings model and mentioning the specific INDEX which has to be used\n",
    "    \n",
    "Vector databases are designed to handle the unique structure of vector embeddings, which are dense vectors of numbers that represent text. They are used in machine learning to capture the meaning of words and map their semantic meaning. <br><br>These databases index vectors for easy search and retrieval by comparing values and finding those that are most similar to one another, making them ideal for natural language processing and AI-driven applications.\n",
    "    <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44afc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821ddd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone import Pinecone as PineconeClient #Importing the Pinecone class from the pinecone package\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "PineconeClient(api_key=PINECONE_API_KEY, environment=\"gcp-starter\")\n",
    "index_name=\"mcq-create\"\n",
    "index = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d84861",
   "metadata": {},
   "source": [
    "## Retrieve Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f46834ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will help us in fetching the top relevent documents from our vector store - Pinecone\n",
    "def get_similiar_docs(query, k=2):\n",
    "    similar_docs = index.similarity_search(query, k=k)\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458a0b1",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "'load_qa_chain' Loads a chain that you can use to do QA over a set of documents.<br>\n",
    "    And we will be using Huggingface for the reasoning purpose\n",
    "<font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4923071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bb574",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a transformer-based large language model.<br> <br>It was created by over 1000 AI researchers to provide a free large language model for everyone who wants to try. Trained on around 366 billion tokens over March through July 2022, it is considered an alternative to OpenAI's GPT-3 with its 176 billion parameters.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4385ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The earlier mentioned 'HuggingFaceHub' class has been depreciated, so please use the below class'HuggingFaceEndpoint' \n",
    "#and the below mentioned model outperforms most of the available open source LLMs\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\") # Model link : https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21fc1a",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Different Types Of Chain_Type:<br><br>\n",
    "\"map_reduce\": It divides the texts into batches, processes each batch separately with the question, and combines the answers to provide the final answer.<br>\n",
    "\"refine\": It divides the texts into batches and refines the answer by sequentially processing each batch with the previous answer.<br>\n",
    "\"map-rerank\": It divides the texts into batches, evaluates the quality of each answer from LLM, and selects the highest-scoring answers from the batches to generate the final answer. These alternatives help handle token limitations and improve the effectiveness of the question-answering process.\n",
    "<font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d37723",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6eee957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will help us get the answer to the question that we raise\n",
    "def get_answer(query):\n",
    "  relevant_docs = get_similiar_docs(query)\n",
    "  print(relevant_docs)\n",
    "  response = chain.invoke(input={\"input_documents\": relevant_docs, \"question\": query})\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c22b2c",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Let's pass our question to the above created function\n",
    "<font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9274da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "our_query = \"How is India's currency?\"\n",
    "answer = get_answer(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf2ccd",
   "metadata": {},
   "source": [
    "## Structure the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f5bce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2559478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d309c532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='question', description='Question generated from provided input text data.', type='string'), ResponseSchema(name='choices', description='Available options for a multiple-choice question in comma separated.', type='string'), ResponseSchema(name='answer', description='Correct answer for the asked question.', type='string')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"question\", description=\"Question generated from provided input text data.\"),\n",
    "    ResponseSchema(name=\"choices\", description=\"Available options for a multiple-choice question in comma separated.\"),\n",
    "    ResponseSchema(name=\"answer\", description=\"Correct answer for the asked question.\")\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32b73a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"question\": string  // Question generated from provided input text data.\n",
      "\t\"choices\": string  // Available options for a multiple-choice question in comma separated.\n",
      "\t\"answer\": string  // Correct answer for the asked question.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# This helps us fetch the instructions the langchain creates to fetch the response in desired format\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    " \n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ChatGPT object google-t5/t5-small\n",
    "chat_model = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b41f0637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c77a73",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "The below snippet will give out a string that contains instructions for how the response should be formatted, and we then insert that into our prompt.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56b47e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"\"\"When a text input is given by the user, please generate multiple choice questions \n",
    "        from it along with the correct answer. \n",
    "        \\n{format_instructions}\\n{user_prompt}\"\"\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca797627",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_query = prompt.format_prompt(user_prompt = answer)\n",
    "print(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ed162b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=final_query.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99f7ca80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```json\n",
      "[\n",
      "\t{\n",
      "\t\t\"question\": \"How is India's currency?\",\n",
      "\t\t\"choices\": \"Indian paisa, Japanese yen, Chinese yuan, Indian rupee (₹)\",\n",
      "\t\t\"answer\": \"Indian rupee (₹)\"\n",
      "\t}\n",
      "]\n",
      "```\n",
      "\n",
      "```json\n",
      "[\n",
      "\t{\n",
      "\t\t\"question\": \"Which landmarks in India are popular tourist destinations?\",\n",
      "\t\t\"choices\": \"Taj Mahal, the Great Wall of China, Statue of Liberty, Jaipur's palaces, Kerala's backwaters, the beaches of Goa\",\n",
      "\t\t\"answer\": \"Taj Mahal, Jaipur's palaces, Kerala's backwaters, the beaches of Goa\"\n",
      "\t}\n",
      "]\n",
      "```\n",
      "\n",
      "```json\n",
      "[\n",
      "\t{\n",
      "\t\t\"question\": \"What is the currency symbol for the Indian rupee?\",\n",
      "\t\t\"choices\": \"₹, $ , ¥, ₩\",\n",
      "\t\t\"answer\": \"₹\"\n",
      "\t}\n",
      "]\n",
      "```\n",
      "\n",
      "```json\n",
      "[\n",
      "\t{\n",
      "\t\t\"question\": \"Which institution controls the issuance of Indian currency?\",\n",
      "\t\t\"choices\": \"Reserve Bank of India, World Bank, International Monetary Fund, Bank of England\",\n",
      "\t\t\"answer\": \"Reserve Bank of India\"\n",
      "\t}\n",
      "]\n",
      "```\n",
      "\n",
      "```json\n",
      "[\n",
      "\t{\n",
      "\t\t\"question\": \"What is India famous for?\",\n",
      "\t\t\"choices\": \"Mountains, deserts, rivers, cuisine, and architecture\",\n",
      "\t\t\"answer\": \"Indian cuisine and architecture\"\n",
      "\t}\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "final_query_output = chat_model.invoke(msg)\n",
    "print(final_query_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f64a5",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "While working with scenarios like above where we have to process multi-line strings(separated by newline characters – ‘\\n’). In such situations, we use re.DOTALL.\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11def92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract JSON data from Markdown text that we have\n",
    "markdown_text = final_query_output\n",
    "json_string = re.search(r'{(.*?)}', markdown_text, re.DOTALL).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4db4592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\"question\": \"How is India's currency?\",\n",
      "\t\t\"choices\": \"Indian paisa, Japanese yen, Chinese yuan, Indian rupee (₹)\",\n",
      "\t\t\"answer\": \"Indian rupee (₹)\"\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "print(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
